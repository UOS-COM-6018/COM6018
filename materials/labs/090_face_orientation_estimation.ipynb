{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 090 Face Orientation Estimation Assignment\n",
    "\n",
    "> COM6018\n",
    "\n",
    "*Copyright &copy; 2023, 2024 Jon Barker, University of Sheffield. All rights reserved*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook have been written to accompany the Face Orientation Estimation assignment. The notebook is unlike the previous labs notebooks, in that it is not a step-by-step guide to a solution. Instead, it contains some notes and snippets of code that you may find useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding the data provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignment makes use of a number of data files that you will need to download from the following location:\n",
    "\n",
    "https://drive.google.com/drive/folders/10y3e2zKkh0lVpRZ3WC21Uu-v-EcbBSYs?usp=sharing\n",
    "\n",
    "As described in the assignment handout, you are provided with the following files:\n",
    "\n",
    "- `train.full.joblib` - the full training dataset\n",
    "- `train.small.joblib` - a smaller subset of the training dataset\n",
    "- `eval1.joblib` - a dataset for evaluating your model\n",
    "- `knn.30.joblib`, `knn.50.joblib`, `knn.90.joblib` - pre-trained kNN models, i.e., a baseline solution\n",
    "\n",
    "If you have not downloaded these already, do so now and store them in the same directory as this notebook.\n",
    "\n",
    "### 2.1 The training data\n",
    "\n",
    "We will first look at the training data. The data can be loaded using the `joblib` library. The training data is stored as a tuple with two elements: a NumPy array of face images and a NumPy array of labels. The code below loads the data. It is also printing out the shape of the arrays to confirm that it has loaded correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "images, face_id = joblib.load('train.small.joblib')\n",
    "print(images.shape, face_id.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the shape of the image arrays is (2500, 125, 94). This means it is a 3-D array storing 2500 images, each of which is 125 pixels high and 94 pixels wide. We can extract a single image using `one_image = images(index)`. The code below extracts the $n$th image and displays it along with the corresponding person ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n = 200\n",
    "plt.matshow(images[n], cmap='gray')  # You can also use 'imshow'\n",
    "print(f'This is an image of person {face_id[n]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 The evaluation data\n",
    "\n",
    "The evaluation data is stored in a nested dictionary structure. The top level dictionary has a key that is the sub-image size (i.e, 30, 50, 90). This can be used to retrieve a dictionary that contains the image and label data, which are called `x_test` and `y_test` to be consistent with the conventions that we've used earlier (i.e., `x` is the input data, and `y` is the output label). The image data (`x_test`) is stored as a 2D NumPy array where each row represents a complete reshaped image. i.e., the set of N-by-N pixel images have been reshaped into rows of N-squared pixels.\n",
    "\n",
    "\n",
    "The code below loads the data and displays the shape of the arrays for the 50 pixel subimages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "eval_data = joblib.load('eval1.joblib')\n",
    "eval_50 = eval_data[50]\n",
    "x_test = eval_50['x_test']\n",
    "y_test = eval_50['y_test']\n",
    "print(x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are 2000 examples in the evaluation set. This breaks down into 500 examples of each of the four orientations. This is a relatively large evaluation set for this task and will enable you to get a fairly accurate estimate of the performance of your model.\n",
    "\n",
    "The code below selects the $n$th images and displays it by reshaping it back into a 2D array and then using the `imshow` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 140\n",
    "sub_image = x_test[n].reshape(50, 50)\n",
    "plt.matshow(sub_image, cmap='gray')  # You can also use 'imshow'\n",
    "print(f'This image is in orientation: {y_test[n]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 The baseline models\n",
    "\n",
    "A set of baseline KNN models have been trained for you and stored in the files `knn.30.joblib`, `knn.50.joblib`, `knn.90.joblib`. These can be loaded using the `joblib` library. The code below loads the 50 pixel model and displays the accuracy on the evaluation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model = joblib.load('knn.50.joblib')\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note above that the print statement provides a description of the model. You can see that it is Pipeline starting with a PCA step that reduces the dimensionality of the data to 50 components. This is followed by a KNN classifier with $k=1$. The performance of this model is not great and it should be easy for you to improve on it.\n",
    "\n",
    "We can check the performance by using the model's score method and passing the evaluation data as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_correct = model.score(x_test, y_test) * 100 \n",
    "print(f'The classifier is {percent_correct:.2f}% correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or is we want to look in more detail, we can compute and display a confusion matrix as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dispaly the confusion matrix using sklearn's display function\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 90, 180, 270])\n",
    "disp.plot()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the above confusion matrix carefully. What do you notice about the errors? Are they random or is there a pattern to them? If there is a pattern, can you explain it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Human Performance\n",
    "\n",
    "For tasks like this, it is interesting to be able to compare the model performance against Human ability. \n",
    "\n",
    "I have provided a simple annotation tool in a file called `tools.py`. Make sure that you have a copy of this file and that it is stored in the same directory as this notebook. \n",
    "\n",
    "The tool will display images from the evaluation set and provide you with a set of buttons that you can use to decide the orientation of the face. The tool will then display the next image. The tool will continue until you choose the 'QUIT' button. The results of your experiment will be saved in a csv file.\n",
    "\n",
    "The tool uses some external dependency that you may need to install using `pip`. This can be done by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyter_ui_poll ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the tool, we first need to make an empty csv file that will be used for saving the results. We will call this file `human.csv`. Run the cell below to perform this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = 'human.csv'\n",
    "with open(results_file, 'w') as f:\n",
    "    f.write(\"n_pixels, index, direction, label\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the annotation tool. We will start by using the 90 pixel sub-images. Run the code below and label about 50 images before hitting 'QUIT'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools\n",
    "import joblib\n",
    "data_eval = joblib.load('eval1.joblib')\n",
    "\n",
    "# Using the 90 pixel sub-images\n",
    "data = data_eval[90]\n",
    "tools.annotate(data['x_test'], data['y_test'], results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A file called 'human.csv' should have appeared in the folder where you are storing the notebook. Have a look at the contents of the folder in an editor. It will show you the label that you selected and the correct label. You can use this to calculate the accuracy of your annotations.\n",
    "\n",
    "Now labels another 50 images but using the 50 pixel images using the cell below (note, results will be appended to the existing `human.csv` file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_eval[50]\n",
    "tools.annotate(data['x_test'], data['y_test'], results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the tool with the 30 pixel sub-images. Some of these might be hard to get right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_eval[30]\n",
    "tools.annotate(data['x_test'], data['y_test'], results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Runnning Python script from the command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far in this module we have been running all our Python code from inside a Jupyter notebook. Jupyter notebooks are very convenient for rapid prototyping, experimentation and demonstrating ideas, but they are less good for training and evaluating models in a reproducible way. For this, it is better to use a Python script that can be run from the command line. This is what you will need to do for the assignment.\n",
    "\n",
    "The code below shows a Python script that loads a model and evaluates it with the evaluation data. Cut and paste the code below and save it in a file called `my)evaluate.py`. Then open a command line window and run the script using the command `python my_evaluate.py`. You should see the same accuracy as you saw when you ran the code in the notebook.\n",
    "\n",
    "(Note, if the code does not work, you may need to install the `joblib` library using `pip`, i.e., run `pip install joblib`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Python script for evaluating a model\n",
    "\n",
    "Cut and paste this code into a file called `evaluate.py` and run it with the command `python evaluate.py`.\n",
    "\"\"\"\n",
    "import joblib\n",
    "\n",
    "def main():\n",
    "    # Load the model for 50 pixel sub-images\n",
    "    model = joblib.load('knn.50.joblib')\n",
    "\n",
    "    # Load the data and select the 50 pixel sub-images\n",
    "    all_data = joblib.load('eval1.joblib')\n",
    "    data = all_data[50]\n",
    "    x_test = data['x_test']\n",
    "    y_test = data['y_test']\n",
    "\n",
    "    # Evaluate the model\n",
    "    percent_correct = model.score(x_test, y_test) * 100\n",
    "    print(f'The classifier is {percent_correct:.2f}% correct')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above should all be very familiar from the code we have seen in the notebooks. Notice how all the code that you want to run appears inside a function called `main`. This is good practice and avoids having variables defined in the global scope. This top level function can be called anything, but it is conventional to call it `main`. \n",
    "\n",
    "Notice how the call to the function `main` occurs at the bottom of the script and is inside an if-statement. This line \"if __name__ == '__main__':\" is a common pattern using a special variable \"__name__\" that is automatically set to \"__main__\" when the script is run from the command line. So this pattern ensures that the code only runs when it is called from the command line. This is useful because it means you can import this code into another script without it running automatically. Do not worry about this too much, but it is good practice to use this pattern.\n",
    "\n",
    "The code above is not very versatile because it always uses the same model file and the same sub-image size. It would be better if we could pass these as command line arguments. Python has a module called `argparse` that makes this easy. The code below shows how to use this module to pass the model file and the evaluation data as command line arguments. Cut and paste the code below and save it in a file called `my_evaluate2.py`. Then open a command line window and run the script using the command `python my_evaluate2.py knn.50.joblib 50`. You should see the same accuracy as you saw when you ran the code in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate a model on the eval1 dataset.\n",
    "\n",
    "Requires the eval1.joblib file to be present in the current directory.\n",
    "\n",
    "Usage:\n",
    "    python evaluate_model.py <model_file> <n_pixels>\n",
    "\n",
    "    model_file: Path to the model file.\n",
    "    n_pixels: Number of pixels used to train the model.\n",
    "\"\"\"\n",
    "from argparse import ArgumentParser\n",
    "from joblib import load\n",
    "\n",
    "\n",
    "def evaluate(model_file, n_pixels):\n",
    "    \"\"\"Evaluate a model on the eval1 dataset.\n",
    "\n",
    "    Args:\n",
    "        model_file (str): Path to the model file.\n",
    "        n_pixels (int): Number of pixels used to train the model.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Evaluating {model_file} with {n_pixels} pixels\")\n",
    "\n",
    "    model = load(model_file)\n",
    "\n",
    "    all_data = load(open(\"eval1.joblib\", \"rb\"))\n",
    "    data = all_data[n_pixels]\n",
    "    x_test = data[\"x_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "\n",
    "    score = model.score(x_test, y_test)\n",
    "    return score\n",
    "\n",
    "def main():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"model_file\", type=str)\n",
    "    parser.add_argument(\"n_pixels\", type=int)\n",
    "    args = parser.parse_args()\n",
    "    score = evaluate(args.model_file, args.n_pixels)\n",
    "    print(\"Score:\", score * 100, \"%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your new version of the script you can now also evaluate the other models and the other sub-image sizes. For example, to evaluate the 30 pixel model, you would run the command `python my_evaluate2.py knn.30.joblib 30`. \n",
    "\n",
    "If you'd trained your own model you could now use the same script. For example, if you'd trained a model called `my_model.joblib` you could evaluate it using the command `python my_evaluate2.py my_model.joblib 50`.\n",
    "\n",
    "Note in the code above that the `main` method handles the parsing of the commandline arguments and then calls a function called `evaluate` to do the useful work. It is good practice to separate the argument parsing from the rest of the code functionality. This makes the code easier to read and understand. It also means that the code can be re-used in other scripts. For example, we may have another script that has its own commandline parsing but which wants to use the evaluate function. This can be easily done by importing the `my_evaluate2.py` script, e.g. with `import evaluate from my_evaluate2`. This would not be possible is the commandline parsing and evaluate code were all mixed together in the `main` function.\n",
    "\n",
    "The `argparse` module is very sophisticated and can handle many different types of commandline arguments. For example, it can handle optional arguments, positional arguments, and arguments with default values. It can also handle arguments that are lists of values. It is easy to learn to use as it has a very good [documentation page](https://docs.python.org/3/library/argparse.html) and there are many examples on the web. The following website has some nicely organised examples that start with simple cases and then build up to more complex usage: https://vnetman.github.io/argparse-recipes/2018/04/21/python-3-argparse-recipe-book.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your final report you have been asked to use LaTeX. LaTeX is a typesetting system that is widely used in academia. It is very good for writing technical documents that contain equations and figures. It is also very good for writing documents that contain references to other documents. You will need to use it for your dissertation, so this is a good opportunity to learn how to use it.\n",
    "\n",
    "We will discuss this more in the lecture session on Monday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LaTeX can be installed on your own computer, or it can also be used online via the website https://www.overleaf.com/. Overleaf is an excellent LaTeX hosting service that has a lot of documentation and examples. It is free to use, but you will need to create an account. Whether you prefer to use Overleaf or install LaTeX on your own computer is up to you. The advantage of running on your own computer is that you can work offline and it can be faster. The advantage of Overleaf is that it is easier to get started and you can work on your report from any computer with an internet connection.\n",
    "\n",
    "## 4.1 Installing LaTeX on your own computer\n",
    "\n",
    "If you want to install LaTeX on your own computer, you can download it from the following website: https://www.latex-project.org/get/. Installation should be very easy for Linux and Mac. LaTeX is also available for Windows, but it is a bit more complicated to install. If you are using Windows, I recommend that you use Overleaf instead.\n",
    "\n",
    "There is a template for the assignment stored in a zip file called 'COM6018_latex_template.zip' in the assigment directory of the module's git repository. Run 'git pull' to make sure that you have it and then copy it to a new working directory and unzip it. Move into the folder 'COM6018_latex_template'. If you have LaTeX installed correctly then you should be able to compile the template document using the command 'pdflatex report.tex'. This should create a file called 'report.pdf' that you can open in a PDF viewer. If you have any problems, please ask for help or use Overleaf instead (next section)\n",
    "\n",
    "## 4.2 Using LaTeX onlin with Overleaf\n",
    "\n",
    "Overleaf is one of several online LaTeX services, others include Papeeria, LaTeX base, CoCalc. Overleaf is the most popular and has the best documentation and examples. It is free to use, but you will need to create an account.\n",
    "\n",
    "To get started with Overleaf, go to the website https://www.overleaf.com/ and create an account. Then click on the 'New Project' button and select 'Upload Project'. Then upload the file `COM6018_latex_template.zip` that you can find in the assignment directory of the module's git repository.\n",
    "\n",
    "After uploading the zip file, the project should open and you will see the LaTeX source on the left and the pdf document that it generates on the right. You can edit the source and the pdf will automatically update. \n",
    "\n",
    "We will spend time on Monday explaining how to use LaTeX. In the meantime, you can look at the source of the template document and try to work out what it is doing. You can also look at the Overleaf documentation and examples. The following page is a good place to start: https://www.overleaf.com/learn/latex/Learn_LaTeX_in_30_minutes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2023, 2024 Jon Barker, University of Sheffield. All rights reserved*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
