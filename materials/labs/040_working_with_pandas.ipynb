{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrZtAsXYnVt-"
      },
      "source": [
        "# 040 Working with Pandas\n",
        "\n",
        "> COM6018\n",
        "\n",
        "*Copyright &copy; 2023, 2024 Jon Barker, University of Sheffield. All rights reserved*.\n",
        "\n",
        "In this lab we are going to be working with Pandas. We will see how it can greatly simplify tasks that were quite challenging when using pure Python. To make this clear we will be starting by repeating the analysis we performed in Lab Class 020, ie. producing a plot of the combined Global Warming Potential of atmospheric methane and carbon dioxide over the last three decades.\n",
        "\n",
        "Start by briefly reviewing the solution notebook for Lab Class 020. Also make sure you have read through the Pandas tutorial notebook that will introduce a lot of the functionality that we will be using in this lab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkkuLlWOnVuC"
      },
      "source": [
        "## Step 1 - Reading in the C02 data\n",
        "\n",
        "Before we can use the Pandas package, we will need to import it. Pandas is conventionally imported with the name 'pd'. Run the cell below which will import both Pandas and NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i5ZdxZxfnVuD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFfz-fwdnVuE"
      },
      "source": [
        "First, we will use the `pd.read_csv` method to load the carbon dioxide data from the CSV file, `data/co2.csv`. The loaded data will be stored in a Pandas dataframe which we will call `co2_df`. The CSV file has six fields which we will name as follows: 'Yr', 'Mn', 'Dy', 'CO2', 'NB', 'scale'. The `co2.csv` file has comment lines starting with a '%' symbol. These can be automatically skipped. If you are unsure how to do this then look again at the tutorial notebook, or check the documentation for `pd.read_csv`.\n",
        "\n",
        "Complete the cell below such that the TEST cell passes correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "8EkYq1nxnVuE",
        "outputId": "b4cc76b1-cda4-48e1-affe-5d98eb09d6c5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/co2.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-153074a27269>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mco2_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/co2.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'yr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Mn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CO2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scale'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/co2.csv'"
          ]
        }
      ],
      "source": [
        "# WRITE SOLUTION HERE\n",
        "import csv\n",
        "\n",
        "co2_df=pd.read_csv('data/co2.csv',comment='%',names=['yr','Mn', 'Dy', 'CO2', 'NB', 'scale'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "hw2NjzU6nVuE",
        "outputId": "c0066170-2ecb-40b7-c90b-e6ccded350c9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'co2_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-79ade6c27de2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mco2_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'co2_df' is not defined"
          ]
        }
      ],
      "source": [
        "print(co2_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "bG0w4TaAnVuF",
        "outputId": "a0c4f428-f31b-4961-c346-d150414ccdba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'co2_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8e67eb1d0a87>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TEST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mco2_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Yr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1960\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Mn'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dy'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CO2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m313.31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NB'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scale'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sta'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'mlo'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All tests passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'co2_df' is not defined"
          ]
        }
      ],
      "source": [
        "# TEST\n",
        "assert dict(co2_df.iloc[1000]) == {'Yr': np.int64(1960), 'Mn': np.int64(9), 'Dy': np.int64(27), 'CO2': np.float64(313.31), 'NB': np.float64(3.0), 'scale': np.float64(12.0), 'sta': 'mlo'}\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBejjhc7nVuF"
      },
      "source": [
        "Note that all the fields have correctly been recognised as numeric types. The year, month and day fields would be more sensibly stored as integers but we are not going to worry about that for now. Note also that reading the data and converting to numeric types has taken one line of code. Compare this to the solution in Lab Class 020 which required quite a lot of code even when using the `csv` module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HZ3JO-rnVuF"
      },
      "source": [
        "## Step 2 - Reading in the CH<sub>4</sub> data\n",
        "\n",
        "We will now read the methane data (i.e., CH<sub>4</sub>) from the file `data/ch4.csv` into a variable which we will call `ch4_df`. This can be done in a similar way but you will need to modify the `read_csv` parameters to match the CSV file contents.\n",
        "\n",
        "Note, the `data/ch4.csv` file has a row that stores the field names so you will not need to use the `names` parameter. Note also that the file uses the value '-999.99' to mean that there is a missing reading. You will need to set the 'na_values' parameter in order to handle this correctly. If in doubt, check the Pandas `read_csv` documentation.\n",
        "\n",
        "Write the code below and then check it by running the test cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lql71LyonVuF"
      },
      "outputs": [],
      "source": [
        "# WRITE SOLUTION HERE\n",
        "ch4_df=pd.read_csv('data/ch4.csv',comment='#',header=0,delimiter=\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtvBmEqknVuF"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "assert dict(ch4_df.iloc[1000]) == {'site_code': 'MLO', 'year': np.int64(1989), 'month': np.int64(9), 'day': np.int64(27), 'hour': np.int64(0), 'minute': np.int64(0), 'second': np.int64(0), 'datetime': '1989-09-27T00:00:00Z', 'time_decimal': np.float64(1989.7369863013696), 'midpoint_time': np.int64(622900800), 'value': np.float64(1696.02), 'value_std_dev': np.float64(7.04), 'nvalue': np.int64(7), 'latitude': np.float64(19.536), 'longitude': np.float64(-155.576), 'altitude': np.float64(3437.0), 'elevation': np.float64(3397.0), 'intake_height': np.float64(40.0), 'qcflag': '...'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VJOgtxlnVuF"
      },
      "source": [
        "## Step 3 - Simplifying the data\n",
        "\n",
        "We are now going to remove the fields that from each data frame that we are not interested in and rename the others. In our transformed dataframes we want just four columns: `year`, `month`, `day` and `co2_concentration` for the carbon dioxide data, and `year`, `month`, `day` and `ch4_concentration` for the methane data.\n",
        "\n",
        "Making a new dataframe from a subset of columns of an existing one is a very common operation. Check the tutorial notes if you cannot remember how to do this. To rename the fields we can use the DataFrame `rename` method. These two steps can be written in a single line of code. (Again, compare this with the equivalent step when we implemented this in Lab Class 020 without using Pandas.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gft61kvOnVuG"
      },
      "outputs": [],
      "source": [
        "# WRITE SOLUTION HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sA5HZUUlnVuG"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "\n",
        "assert dict(co2_df.iloc[1000]) == {'year': np.float64(1960.0), 'month': np.float64(9.0), 'day': np.float64(27.0), 'co2_concentration': np.float64(313.31)}\n",
        "assert dict(ch4_df.iloc[1000]) == {'year': np.float64(1989.0), 'month': np.float64(9.0), 'day': np.float64(27.0), 'ch4_concentration': np.float64(1696.02)}\n",
        "print('All tests passed!')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "julpsbzmnVuG"
      },
      "source": [
        "## Step 4 - Dealing with missing values\n",
        "\n",
        "In the next step we will remove any rows from the dataframes that contain missing concentration values. Check the notes on selecting rows. Hint: you can use the `isna()` method which returns True for data entries that have NaN values, or, better, you can use the special purpose `dropna()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqnKDKTfnVuH"
      },
      "outputs": [],
      "source": [
        "# WRITE SOLUTION HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7w1bnTonVuH"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "assert len(co2_df) == 18056\n",
        "assert len(ch4_df) == 12728\n",
        "print('All tests passed!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd9_QFuwnVuH"
      },
      "source": [
        "## Step 5 - Fixing the units\n",
        "\n",
        "If you remember from Lab 2, the methane concentrations are stored as parts per billion, whereas the carbon dioxide is in parts per million. It will be more convenient to have them both in the same units (i.e., parts per million, *ppm*). This can be done simply by dividing the `ch4_concentration` values by 1000.\n",
        "\n",
        "Scaling a single column of a dataframe is very easy and can be done with a single line. Do this in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEbOs3_nnVuH"
      },
      "outputs": [],
      "source": [
        "# WRITE SOLUTION HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5T8W4vwnVuI"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "print(ch4_df.ch4_concentration[92])\n",
        "print('All tests passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nASISSf5nVuI"
      },
      "source": [
        "## Step 6 - Joining the data\n",
        "\n",
        "We will now merge our dataframes into a single dataframe using the `merge` method.\n",
        "\n",
        "We want to ignore any rows for which there is not both a CO<sub>2</sub> and a CH<sub>4</sub> measurement, i.e., if for a given day either the CO<sub>2</sub> *or* CH<sub>4</sub> measurement is missing then that day will not appear in the merged dataframe.  This is called an 'inner' join. We can specify that we want an 'inner' join by setting the `how` parameter to `inner` when we call the `merge` method.  \n",
        "\n",
        "When performing a merge we need a unique identifier for each row. In our case we will use the combination of the 'year', 'month' and 'day' fields.  This is specified using the `on` parameter.\n",
        "\n",
        "The `merge` method returns a new dataframe which we will store in a variable called `df`.\n",
        "\n",
        "This step can be completed with a single line. Check the notes if you are unsure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGTTvSwLnVuI"
      },
      "outputs": [],
      "source": [
        "ch4_df.dtypes, co2_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi9HzbFvnVuI"
      },
      "outputs": [],
      "source": [
        "# WRITE SOLUTION HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mReilyZenVuI"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "assert df['co2_concentration'][0] == 350.84\n",
        "assert df['ch4_concentration'][0] == 1.70002\n",
        "print('All tests passed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghb4Z_CZnVuI"
      },
      "source": [
        "Another advantage of using Pandas is that it is very fast. In the cell below, '%timeit' is used to time the `merge` command. On my machine is takes about 2 ms. This is faster than the function that we wrote in lab 2 when implementing the `merge` using Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QictxM3nVuI"
      },
      "outputs": [],
      "source": [
        "%timeit co2_df.merge(ch4_df, how='inner', on=['day', 'month', 'year'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D46JyzztnVuI"
      },
      "source": [
        "## Step 7 - Processing the data\n",
        "\n",
        "We will continue to follow the steps from Lab 020. We will now combine the CO<sub>2</sub> and CH<sub>4</sub> concentrations into a single value to represent the total 'Global Warming Potential' (GWP) of the gases. Remember, we can do this by converting all gas concentrations into 'CO<sub>2</sub> equivalent' (CO<sub>2</sub>e) and then summing them. For CH<sub>4</sub> we need to multiple by 28 to compute it's CO<sub>2</sub> equivalent.\n",
        "\n",
        "So we will make a new column in the dataframe that we will call `co2e` and which will be equal to `co2_concentration + 28 * ch4_concentration`. This should be a single line of code. Refer to the notes if you are unsure how to do it. Write this in the cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5JyYM8InVuI"
      },
      "outputs": [],
      "source": [
        "# WRITE SOLUTION HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60HTbO_hnVuJ"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "assert df.co2e[0] == 398.44056\n",
        "print('All tests passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLvguSe_nVuJ"
      },
      "source": [
        "## Step 8 -- Plotting the data\n",
        "\n",
        "### Step 8.1 -- Plotting CO<sub>2</sub>e concentration over time\n",
        "\n",
        "We are now ready to plot the data. We want to see a graph of the CO<sub>2</sub>e concentration over time.\n",
        "\n",
        "Previously, in Lab 2, we used the matplotlib library to make the plot. Pandas makes things a bit easier as it has its own plotting tools builtin. The Data Series class has a plot method. e.g. to plot the `co2e` data we can use `df['co2e'].plot()` or, equivalently, `df.co2e.plot()`. The plot method has many parameters that can be used to control the appearance of the plot.\n",
        "\n",
        "Make a plot with the title 'CO2e over time' and with the y-axis labeled as 'CO2e (ppm)' and the x-axis labeled as 'Year'\n",
        "\n",
        "You will find that the x-axis will be the index of the row, ie from 0 to 9375. We would like it to be the year. Don't worry about this for now, we will fix it in the next step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_CdMkDYnVuJ"
      },
      "outputs": [],
      "source": [
        "# WRITE SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLjgujOunVuJ"
      },
      "source": [
        "### Step 8.2 -- Plotting by decimal year\n",
        "\n",
        "In order to get the x-axis right, we need to turn the date into a decimal number that we can plot against.\n",
        "\n",
        "Following what we did in Lab 2, we will use the following equation to convert the year, month and day into a single decimial year value,\n",
        "\n",
        "`decimal_year = year + (month - 1) / 12 + (day - 1) / 365`\n",
        "\n",
        "(Note, this equation is not quite correct as it assumes that each month is the same length, but it is good enough for our purposes.)\n",
        "\n",
        "Add a new column to the DataFrame called `decimal_year` that is formed using the above equation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ2R_-KYnVuJ"
      },
      "outputs": [],
      "source": [
        "# WRITE SOLUTION HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X88HWPzInVuJ"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "assert df.decimal_year[0] == 1987.2554794520547\n",
        "print('All tests passed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vmxu_uBQnVuJ"
      },
      "source": [
        "We can now plot the `co2_e` values against decimal year. To plot one column against another we can use the `plot` method of the dataframe itself and use the 'x' and 'y' parameters: this looks like `df.plot(x='decimial_year, y='co2e')`\n",
        "\n",
        "Try this below, but remember to also give the plot a title and axes labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHBmaPLbnVuK"
      },
      "outputs": [],
      "source": [
        "# WRITE SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KsUAo9cnVuK"
      },
      "source": [
        "We have now used Pandas to repeat everything we did in Lab class 020. Although it has taken a similar number of steps, each step could be completed with a single line of code using pre-existing Pandas functions. Each step will also have run faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hmiqOzRnVuK"
      },
      "source": [
        "## Step 9 - Extending the analysis\n",
        "\n",
        "In this last step we are going to practice what we have just done by adding in data for two more global warming gases: nitrous oxide (N<sub>2</sub>O) and sulphur hexaflouride (SF<sub>6</sub>). The data for these gases is stored in the file `data/n2o.csv` and `data/sf6.csv`.\n",
        "\n",
        "Using the techniques from above, read these files and add two new columns to the dataframe, i.e. `n2o_concentration` and `sf6_concentration` storing concentrations in parts per million (ppm).\n",
        "\n",
        "Be careful to interpret the data correctly, i.e. the nitrous oxide data is recorded in parts per billion and the sulfur hexaflouride is measured as parts per trillion (i.e., 10^9). When computing the CO<sub>2</sub>e figure you can use the following multipliers: 265 for nitrous oxide and 23,500 for sulphur hexaflouride. (These values have been taken from the (IPCC fifth assessment report)[https://ghgprotocol.org/sites/default/files/ghgp/Global-Warming-Potential-Values%20%28Feb%2016%202016%29_1.pdf])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k461AkGonVuK"
      },
      "outputs": [],
      "source": [
        "# WRITE SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UYRuQ4nnVuL"
      },
      "source": [
        "Now make a plot of the total CO2e."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnYtiXdXnVuL"
      },
      "outputs": [],
      "source": [
        "# WRITE SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRIdHu3RnVuL"
      },
      "source": [
        "You can plot multiple series on the same axes by setting the plot method's y parameter to be a list of the names of the series you want to plot, e.g., `y=['co2e', 'co2_concentration']`. Make a plot that shows the CO<sub>2</sub>e value for each of the four gases and the total CO<sub>2</sub>e, all plotted on the same axes. (Hint: in order to do this, you will first need to add new columns to the dataframe to store the CO<sub>2</sub>e of each of the separate gases.)\n",
        "\n",
        "Write your code in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfCiPA4inVuL"
      },
      "outputs": [],
      "source": [
        "# WRITE SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edyw3cmunVuL"
      },
      "source": [
        "You should see from the plot that although the total CO<sub>2</sub>e has been steadily rising, nearly all of the increase is due to co2. i.e., the total CO<sub>2</sub>e line runs in parallel to the CO<sub>2</sub> line.\n",
        "\n",
        "If all have gone well you should have produced a plot looking like the one below.\n",
        "\n",
        "[final plot](figures/co2e_plot.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk1X6pV4nVuL"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this lab, we learned how to use Pandas to load and process data from CSV files. We also learned how to merge data from multiple sources into a single dataframe, how to construct new DataSeries from existing ones, and how to plot the data using matplotlib.\n",
        "\n",
        "All of these steps could have been done without Pandas, indeed, we did most of this in Lab 020 just using Python and standard library module. However, using Pandas has made the process considerably simpler to develop and faster to execute. Pandas also contains a lot of more sophisticated functionality that we have not covered here, but which is easy to learn (or look up!) once you have understood the basics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bY1I6mnnVuM"
      },
      "source": [
        "*Copyright &copy; 2023, 2024 Jon Barker, University of Sheffield. All rights reserved*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn5_56J-nVuM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}